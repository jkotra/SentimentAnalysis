{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Twitter](https://i.imgur.com/ELyYIf9.png)\n",
    "\n",
    "# Twitter Sentiment Analysis\n",
    "\n",
    "Using twitter dataset - sentiment140 that was annotated automatically,we try to train a neural neural network that would produce a positive score for a tweet(a score of **1.0** is the maximum positiveness). this score can be used in a lot of other machine learning models to further it's accuracy.\n",
    "\n",
    "Tools used:\n",
    "\n",
    "- Tensorflow 2.0 (Code optimized for GPU)\n",
    "- Pandas\n",
    "- Scikit-earn\n",
    "- tqdm\n",
    "- Glove pre-trained word embeddings. (twitter specific)\n",
    "\n",
    "Dataset: http://help.sentiment140.com/for-students\n",
    "\n",
    "Glove: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import mmap\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:\n",
    "\n",
    "- Load data\n",
    "- Shuffle\n",
    "- out of 140000 tweets, 800000 are negative and remaining are positive.the negative rows are in first rows.\n",
    "- We shuffle the data such that we have a distribution when in a given sample,there are roughly the same amount of positive and negative tweets.this also makes it easy to use partial dataset to overcome memory constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>Query</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>623516</th>\n",
       "      <td>0</td>\n",
       "      <td>2229581957</td>\n",
       "      <td>Thu Jun 18 15:55:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xDrugFreeMattx</td>\n",
       "      <td>I'm soo fucking sore ughhh  soo fucking worth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232576</th>\n",
       "      <td>0</td>\n",
       "      <td>1979194550</td>\n",
       "      <td>Sun May 31 02:00:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stevieenglish</td>\n",
       "      <td>my tips have gone to shit,  @mellalicious seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523838</th>\n",
       "      <td>0</td>\n",
       "      <td>2193270699</td>\n",
       "      <td>Tue Jun 16 08:13:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lacebound</td>\n",
       "      <td>@twelfthminute Loser, I am doing E Math. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892369</th>\n",
       "      <td>4</td>\n",
       "      <td>1691162833</td>\n",
       "      <td>Sun May 03 17:14:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>K_MAE</td>\n",
       "      <td>just woke up from a wonderful nap. I feel bett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186310</th>\n",
       "      <td>0</td>\n",
       "      <td>1968292428</td>\n",
       "      <td>Fri May 29 21:08:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>nomibear</td>\n",
       "      <td>@alwayskatharine  It works for me... Maybe it'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date     Query  \\\n",
       "623516       0  2229581957  Thu Jun 18 15:55:21 PDT 2009  NO_QUERY   \n",
       "232576       0  1979194550  Sun May 31 02:00:54 PDT 2009  NO_QUERY   \n",
       "523838       0  2193270699  Tue Jun 16 08:13:46 PDT 2009  NO_QUERY   \n",
       "892369       4  1691162833  Sun May 03 17:14:20 PDT 2009  NO_QUERY   \n",
       "186310       0  1968292428  Fri May 29 21:08:32 PDT 2009  NO_QUERY   \n",
       "\n",
       "              username                                              tweet  \n",
       "623516  xDrugFreeMattx  I'm soo fucking sore ughhh  soo fucking worth ...  \n",
       "232576   stevieenglish  my tips have gone to shit,  @mellalicious seem...  \n",
       "523838       lacebound  @twelfthminute Loser, I am doing E Math. I hat...  \n",
       "892369           K_MAE  just woke up from a wonderful nap. I feel bett...  \n",
       "186310        nomibear  @alwayskatharine  It works for me... Maybe it'...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"target\",\"id\",\"date\",\"Query\",\"username\",\"tweet\"]\n",
    "train = pd.read_csv(\"/home/jagadeesh/Datasets/Twitter/train.csv\",encoding = \"ISO-8859-1\",names=cols, header=None)\n",
    "train = shuffle(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 'Query' and 'id'\n",
    "train.drop(['Query','id'],axis=1,inplace=True)\n",
    "\n",
    "#replace 4(positive) with 1\n",
    "train.target.replace(4,1,inplace=True)\n",
    "\n",
    "#Select 10k as validation set\n",
    "val = train[100000:120000]\n",
    "\n",
    "#first 100k as training set.\n",
    "train = train[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 50164 Positive: 49836\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative:\",len(train[train.target == 0].index),\"Positive:\",len(train[train.target == 1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target      0\n",
       "date        0\n",
       "username    0\n",
       "tweet       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for missing values\n",
    "\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set target as label\n",
    "train_labels = train.target.values\n",
    "val_labels = val.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Tokenizing' is nothing but splitting a sentence into a list of words. Similar to ```.split()```\n",
    "- Ex: ```\"Hi, How are you?\"``` --> *Tokenize()* ---> ```[\"Hi,\",\"How\",\"are\",\"you\",\"?\"]```\n",
    "\n",
    "- ``` texts_to_sequences ``` converts tokenized list of words into numeric representation.\n",
    "\n",
    "### glossary\n",
    "\n",
    "- ```word_index``` - Unique words in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 112644\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(pd.concat([train,val])['tweet'])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "\n",
    "train_encoded_docs = t.texts_to_sequences(train.tweet)\n",
    "val_encoded_docs = t.texts_to_sequences(val.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```get_embedding_index``` loads glove data.\n",
    "- ```create_weight_matrix``` creates weights from above *embedding_index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd756eded72a45d68432333a57624d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1193514), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines\n",
    "\n",
    "def get_embedding_index(path):\n",
    "    \n",
    "    embeddings_index = dict()\n",
    "    glove_path = path\n",
    "    f = open(glove_path)\n",
    "    for line in tqdm.tqdm_notebook(f,total=get_num_lines(glove_path)):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def create_weight_matrix(embeddings_index):\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embeddings_index = get_embedding_index(\"/home/jagadeesh/Weights/glove_twitter/glove.twitter.27B.100d.txt\")\n",
    "embedding_matrix = create_weight_matrix(embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Keras doc pad_seq'](https://i.imgur.com/bD07tF7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10\n",
    "\n",
    "train_padded_docs = pad_sequences(train_encoded_docs, maxlen=max_length, padding='post')\n",
    "val_padded_docs = pad_sequences(val_encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "- Embedding(100)\n",
    "- LSTM(50)\n",
    " - This layers speeds up training significantly.\n",
    "- Dense(100)\n",
    "- Dropout(0.2)\n",
    "- Dense(1) (Output's score in range of 0 to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, 100,weights=[embedding_matrix], input_length=10))\n",
    "model.add(tf.compat.v1.keras.layers.CuDNNLSTM(50))\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 100)           11264400  \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       (None, 50)                30400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 11,300,001\n",
      "Trainable params: 11,300,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/15\n",
      "100000/100000 [==============================] - 5s 47us/sample - loss: 0.6908 - acc: 0.5410 - val_loss: 0.6858 - val_acc: 0.5711\n",
      "Epoch 2/15\n",
      "100000/100000 [==============================] - 1s 15us/sample - loss: 0.6710 - acc: 0.6342 - val_loss: 0.6456 - val_acc: 0.6812\n",
      "Epoch 3/15\n",
      "100000/100000 [==============================] - 2s 15us/sample - loss: 0.5897 - acc: 0.7225 - val_loss: 0.5646 - val_acc: 0.7124\n",
      "Epoch 4/15\n",
      "100000/100000 [==============================] - 2s 17us/sample - loss: 0.4998 - acc: 0.7644 - val_loss: 0.5221 - val_acc: 0.7425\n",
      "Epoch 5/15\n",
      "100000/100000 [==============================] - 1s 15us/sample - loss: 0.4317 - acc: 0.8107 - val_loss: 0.5193 - val_acc: 0.7493\n",
      "Epoch 6/15\n",
      "100000/100000 [==============================] - 1s 6us/sample - loss: 0.3816 - acc: 0.8371 - val_loss: 0.5232 - val_acc: 0.7514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe70018f550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"/home/jagadeesh/Weights/twitter/model\",save_best_only=True)\n",
    "model.fit(train_padded_docs, train_labels,validation_data=(val_padded_docs,val_labels), epochs=15, verbose=1,batch_size=10000,callbacks=[es,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(val_padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered = pd.DataFrame(val['tweet'])\n",
    "infered['score'] = predicted\n",
    "infered['result'] = infered.score.apply(lambda x: \"+\" if x > 0.51 else \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>score</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570472</th>\n",
       "      <td>@NatalieGear totez jel. Neither of my classes end for like 3 more weeks</td>\n",
       "      <td>0.119479</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799072</th>\n",
       "      <td>It's soooo hot. I don't deal with heat well.</td>\n",
       "      <td>0.182169</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381948</th>\n",
       "      <td>Woke 07:23 to a rain-teem hard at work, now taking an extended break under two layers of patchwork clouds, not warm, not cold, not windy</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456231</th>\n",
       "      <td>brb for like 5 minutes i have to do the dishes</td>\n",
       "      <td>0.219266</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574601</th>\n",
       "      <td>@XtnDvla I am eating sushi. Home made.</td>\n",
       "      <td>0.838343</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909881</th>\n",
       "      <td>@MissLaniSasha lmfao!!! Yea anyone?? Please?? We're really pretty!!!</td>\n",
       "      <td>0.952968</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37124</th>\n",
       "      <td>Awake. Very very very tired and i think i have to move my ass out of the bed. Do some morning sport.</td>\n",
       "      <td>0.473959</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949242</th>\n",
       "      <td>Apple store coming to Roseville!</td>\n",
       "      <td>0.341650</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099968</th>\n",
       "      <td>@WritingTravel thanks for the follow friday</td>\n",
       "      <td>0.991828</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079704</th>\n",
       "      <td>@CJay282 gotta sweat it out ma. the swine flu is receding tho.</td>\n",
       "      <td>0.060083</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994616</th>\n",
       "      <td>Introducing another MAS voice on twitter: @MAStravel  Watch out for travel-related tweets from this account!</td>\n",
       "      <td>0.431553</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289165</th>\n",
       "      <td>is going into my fantasy world for a few hours. I wish i can stay there forever  its much nicer then realty</td>\n",
       "      <td>0.566429</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203618</th>\n",
       "      <td>damn i missed everything when i did my lesson...</td>\n",
       "      <td>0.023360</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668718</th>\n",
       "      <td>People are oftly quiet on here today</td>\n",
       "      <td>0.262287</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036108</th>\n",
       "      <td>@AlexandraMarion Thank you Alexa!!</td>\n",
       "      <td>0.991247</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556234</th>\n",
       "      <td>Car pooling-just waiting 4 my ride home</td>\n",
       "      <td>0.346346</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196849</th>\n",
       "      <td>Didn't get a Wii yesterday, didn't have enough time...  Maybe today... Maybe</td>\n",
       "      <td>0.263511</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319066</th>\n",
       "      <td>my little dog's having surgery now.  i don't get to see her until late tonight.</td>\n",
       "      <td>0.136173</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682565</th>\n",
       "      <td>@Tyyylerr Its tiny, and stings.  I cant pluck my right eyebrow now.</td>\n",
       "      <td>0.119374</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545364</th>\n",
       "      <td>Flip flop shopping(: RIP my old black ones</td>\n",
       "      <td>0.042073</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984675</th>\n",
       "      <td>IM TALKING TO CASS AND EMMA  HEY GUYS (:</td>\n",
       "      <td>0.840433</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345083</th>\n",
       "      <td>@Stephanya moving further away from moi.</td>\n",
       "      <td>0.615876</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525692</th>\n",
       "      <td>@anirbanroy Hi! good to see you tweeting after long!</td>\n",
       "      <td>0.972210</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622841</th>\n",
       "      <td>updated my iPhone to 3.0 and the ESPN app stopped working</td>\n",
       "      <td>0.079843</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468594</th>\n",
       "      <td>Rocked Ladykillerz yesterday!Now 1gig with Sneakerz @ BLM 9,Line-up: Lucien Foort,Oliver Twizt,Youri Dounatz, Mo Mc and more.. be there!</td>\n",
       "      <td>0.481557</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310261</th>\n",
       "      <td>@BeaMarqz awwww  well i hope im with at least one of you #whatsmysection #whatsmysection</td>\n",
       "      <td>0.693740</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307613</th>\n",
       "      <td>FUCK SQUIRREL EYE SOCKETS! my thesis sounds like fart DX ahhh im gunna fail history</td>\n",
       "      <td>0.076627</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236634</th>\n",
       "      <td>Grr. Internet failures. Can access LJ friends page but not individual journals. BBC News but not Guardian! So confusing and frustrating.</td>\n",
       "      <td>0.028417</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104137</th>\n",
       "      <td>Blaring Minus the Bear in my room now.</td>\n",
       "      <td>0.556504</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740868</th>\n",
       "      <td>Photo: thisiswhyyourefat: Someone, bring me to try ravioli  http://tumblr.com/xvm23xwil</td>\n",
       "      <td>0.809510</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193706</th>\n",
       "      <td>#myweakness = that i am lazy sometimes</td>\n",
       "      <td>0.935733</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393649</th>\n",
       "      <td>just dropped them off at the  airport</td>\n",
       "      <td>0.123313</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535332</th>\n",
       "      <td>big ups to @FabulousMady for reminding me FB has personal urls now. I see you trend setter.</td>\n",
       "      <td>0.666380</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149746</th>\n",
       "      <td>just went out to dinner with anna and her parents we had a totall blast and were flicking food with our chopsticks</td>\n",
       "      <td>0.890121</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562730</th>\n",
       "      <td>@MonicaJORourke   I'll keep at it...Ugh, titles can be so hard. If I ever had a child, it would grow up and die before it got named.</td>\n",
       "      <td>0.779617</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37070</th>\n",
       "      <td>apple kept saying that  my signiture is invalid</td>\n",
       "      <td>0.055882</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788590</th>\n",
       "      <td>Have to take dh to hospital today due to abdomenal pain  I have 1 hour to clean my house for my mom....</td>\n",
       "      <td>0.215035</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32915</th>\n",
       "      <td>I hate shaving. I miss the days before puberty with my little peach fuzz . good times... if i dont shave. I start to look like a bum</td>\n",
       "      <td>0.188577</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154703</th>\n",
       "      <td>is up hope its another scorcher!</td>\n",
       "      <td>0.163508</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374928</th>\n",
       "      <td>No, Black Eyed Peas, I don't want your Boom Boom Pow</td>\n",
       "      <td>0.860014</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674553</th>\n",
       "      <td>@thenamesmary is he talking about karen?</td>\n",
       "      <td>0.946840</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030590</th>\n",
       "      <td>all of the supporters. The people applauding from restaurants, it all makes me want to cry</td>\n",
       "      <td>0.096751</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92910</th>\n",
       "      <td>I hate cleaning             -S u n n y</td>\n",
       "      <td>0.052555</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587287</th>\n",
       "      <td>Fell in love with Cane Hill again on Sunday, good memories on why it was such a great place.  Also now the owner of Cane Hill Hosp. shirt</td>\n",
       "      <td>0.593656</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558316</th>\n",
       "      <td>@gilbirmingham Hope you get all the followers you nedd</td>\n",
       "      <td>0.913829</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109592</th>\n",
       "      <td>Its mate is all upset, too...</td>\n",
       "      <td>0.104988</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208183</th>\n",
       "      <td>@omewan it does help  and that song is class ;)</td>\n",
       "      <td>0.914168</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313774</th>\n",
       "      <td>Sitting on the cabernet couch... Nice</td>\n",
       "      <td>0.941598</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303119</th>\n",
       "      <td>@AliaTheArtSnob bushy tailed? sounds neat  i think paris took its toll on you O_O</td>\n",
       "      <td>0.201665</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857300</th>\n",
       "      <td>chillin</td>\n",
       "      <td>0.935752</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              tweet  \\\n",
       "570472   @NatalieGear totez jel. Neither of my classes end for like 3 more weeks                                                                      \n",
       "799072   It's soooo hot. I don't deal with heat well.                                                                                                 \n",
       "1381948  Woke 07:23 to a rain-teem hard at work, now taking an extended break under two layers of patchwork clouds, not warm, not cold, not windy     \n",
       "456231   brb for like 5 minutes i have to do the dishes                                                                                               \n",
       "1574601  @XtnDvla I am eating sushi. Home made.                                                                                                       \n",
       "909881   @MissLaniSasha lmfao!!! Yea anyone?? Please?? We're really pretty!!!                                                                         \n",
       "37124    Awake. Very very very tired and i think i have to move my ass out of the bed. Do some morning sport.                                         \n",
       "949242   Apple store coming to Roseville!                                                                                                             \n",
       "1099968  @WritingTravel thanks for the follow friday                                                                                                  \n",
       "1079704  @CJay282 gotta sweat it out ma. the swine flu is receding tho.                                                                               \n",
       "994616   Introducing another MAS voice on twitter: @MAStravel  Watch out for travel-related tweets from this account!                                 \n",
       "289165   is going into my fantasy world for a few hours. I wish i can stay there forever  its much nicer then realty                                  \n",
       "203618   damn i missed everything when i did my lesson...                                                                                             \n",
       "668718   People are oftly quiet on here today                                                                                                         \n",
       "1036108  @AlexandraMarion Thank you Alexa!!                                                                                                           \n",
       "1556234  Car pooling-just waiting 4 my ride home                                                                                                      \n",
       "196849   Didn't get a Wii yesterday, didn't have enough time...  Maybe today... Maybe                                                                 \n",
       "319066   my little dog's having surgery now.  i don't get to see her until late tonight.                                                              \n",
       "682565   @Tyyylerr Its tiny, and stings.  I cant pluck my right eyebrow now.                                                                          \n",
       "545364   Flip flop shopping(: RIP my old black ones                                                                                                   \n",
       "984675   IM TALKING TO CASS AND EMMA  HEY GUYS (:                                                                                                     \n",
       "345083   @Stephanya moving further away from moi.                                                                                                     \n",
       "1525692  @anirbanroy Hi! good to see you tweeting after long!                                                                                         \n",
       "622841   updated my iPhone to 3.0 and the ESPN app stopped working                                                                                    \n",
       "1468594  Rocked Ladykillerz yesterday!Now 1gig with Sneakerz @ BLM 9,Line-up: Lucien Foort,Oliver Twizt,Youri Dounatz, Mo Mc and more.. be there!     \n",
       "310261   @BeaMarqz awwww  well i hope im with at least one of you #whatsmysection #whatsmysection                                                     \n",
       "307613   FUCK SQUIRREL EYE SOCKETS! my thesis sounds like fart DX ahhh im gunna fail history                                                          \n",
       "236634   Grr. Internet failures. Can access LJ friends page but not individual journals. BBC News but not Guardian! So confusing and frustrating.     \n",
       "104137   Blaring Minus the Bear in my room now.                                                                                                       \n",
       "740868   Photo: thisiswhyyourefat: Someone, bring me to try ravioli  http://tumblr.com/xvm23xwil                                                      \n",
       "193706   #myweakness = that i am lazy sometimes                                                                                                       \n",
       "393649   just dropped them off at the  airport                                                                                                        \n",
       "1535332  big ups to @FabulousMady for reminding me FB has personal urls now. I see you trend setter.                                                  \n",
       "1149746  just went out to dinner with anna and her parents we had a totall blast and were flicking food with our chopsticks                           \n",
       "1562730  @MonicaJORourke   I'll keep at it...Ugh, titles can be so hard. If I ever had a child, it would grow up and die before it got named.         \n",
       "37070    apple kept saying that  my signiture is invalid                                                                                              \n",
       "788590   Have to take dh to hospital today due to abdomenal pain  I have 1 hour to clean my house for my mom....                                      \n",
       "32915    I hate shaving. I miss the days before puberty with my little peach fuzz . good times... if i dont shave. I start to look like a bum         \n",
       "1154703  is up hope its another scorcher!                                                                                                             \n",
       "374928   No, Black Eyed Peas, I don't want your Boom Boom Pow                                                                                         \n",
       "674553   @thenamesmary is he talking about karen?                                                                                                     \n",
       "1030590  all of the supporters. The people applauding from restaurants, it all makes me want to cry                                                   \n",
       "92910    I hate cleaning             -S u n n y                                                                                                       \n",
       "1587287  Fell in love with Cane Hill again on Sunday, good memories on why it was such a great place.  Also now the owner of Cane Hill Hosp. shirt    \n",
       "1558316  @gilbirmingham Hope you get all the followers you nedd                                                                                       \n",
       "109592   Its mate is all upset, too...                                                                                                                \n",
       "1208183  @omewan it does help  and that song is class ;)                                                                                              \n",
       "1313774  Sitting on the cabernet couch... Nice                                                                                                        \n",
       "1303119  @AliaTheArtSnob bushy tailed? sounds neat  i think paris took its toll on you O_O                                                            \n",
       "857300   chillin                                                                                                                                      \n",
       "\n",
       "            score result  \n",
       "570472   0.119479  -      \n",
       "799072   0.182169  -      \n",
       "1381948  0.026420  -      \n",
       "456231   0.219266  -      \n",
       "1574601  0.838343  +      \n",
       "909881   0.952968  +      \n",
       "37124    0.473959  -      \n",
       "949242   0.341650  -      \n",
       "1099968  0.991828  +      \n",
       "1079704  0.060083  -      \n",
       "994616   0.431553  -      \n",
       "289165   0.566429  +      \n",
       "203618   0.023360  -      \n",
       "668718   0.262287  -      \n",
       "1036108  0.991247  +      \n",
       "1556234  0.346346  -      \n",
       "196849   0.263511  -      \n",
       "319066   0.136173  -      \n",
       "682565   0.119374  -      \n",
       "545364   0.042073  -      \n",
       "984675   0.840433  +      \n",
       "345083   0.615876  +      \n",
       "1525692  0.972210  +      \n",
       "622841   0.079843  -      \n",
       "1468594  0.481557  -      \n",
       "310261   0.693740  +      \n",
       "307613   0.076627  -      \n",
       "236634   0.028417  -      \n",
       "104137   0.556504  +      \n",
       "740868   0.809510  +      \n",
       "193706   0.935733  +      \n",
       "393649   0.123313  -      \n",
       "1535332  0.666380  +      \n",
       "1149746  0.890121  +      \n",
       "1562730  0.779617  +      \n",
       "37070    0.055882  -      \n",
       "788590   0.215035  -      \n",
       "32915    0.188577  -      \n",
       "1154703  0.163508  -      \n",
       "374928   0.860014  +      \n",
       "674553   0.946840  +      \n",
       "1030590  0.096751  -      \n",
       "92910    0.052555  -      \n",
       "1587287  0.593656  +      \n",
       "1558316  0.913829  +      \n",
       "109592   0.104988  -      \n",
       "1208183  0.914168  +      \n",
       "1313774  0.941598  +      \n",
       "1303119  0.201665  -      \n",
       "857300   0.935752  +      "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "infered.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try it on our own sentence.\n",
    "\n",
    "pos1 = \"I'm feeling great today, thanks for asking!\"\n",
    "pos2 = \"even though i \"\n",
    "\n",
    "neg1 = \"very frustated with recent development in my country.\"\n",
    "neg2 = \"\"\n",
    "\n",
    "combined_arr = np.array([pos,neg])\n",
    "\n",
    "sentences = t.texts_to_sequences(combined_arr)\n",
    "\n",
    "sentences = pad_sequences(sentences,maxlen=max_length,padding='post')\n",
    "\n",
    "res = model.predict(sentences)\n",
    "\n",
    "res_df = pd.DataFrame(combined_arr)\n",
    "res_df['analysis'] = res\n",
    "res_df['result'] = res_df.analysis.apply(lambda x: \"+\" if x > 0.51 else \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>analysis</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm feeling great today, thanks for asking!</td>\n",
       "      <td>0.976719</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>very frustated with recent development in my country.</td>\n",
       "      <td>0.665812</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  analysis result\n",
       "0  I'm feeling great today, thanks for asking!            0.976719  +    \n",
       "1  very frustated with recent development in my country.  0.665812  +    "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
